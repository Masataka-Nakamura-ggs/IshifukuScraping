# Ishifuku Gold Price Scraper v2 - å®Œå…¨ç‰ˆã‚¬ã‚¤ãƒ‰

## ğŸ“‹ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦

çŸ³ç¦é‡‘å±ã‹ã‚‰ã®é‡‘ä¾¡æ ¼ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ã®å®Œå…¨ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆã§ã™ã€‚é«˜ã„ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ï¼ˆ84%ï¼‰ã€CI/CDçµ±åˆã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã€ã‚¢ãƒ©ãƒ¼ãƒˆæ©Ÿèƒ½ã‚’å‚™ãˆãŸæœ¬æ ¼çš„ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã—ã¦ç”Ÿã¾ã‚Œå¤‰ã‚ã‚Šã¾ã—ãŸã€‚

## âœ¨ ä¸»è¦ãªæ”¹å–„ç‚¹

### ğŸ—ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å®Œå…¨åˆ·æ–°
- **ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼è¨­è¨ˆ**: æ©Ÿèƒ½åˆ¥ã«åˆ†é›¢ã•ã‚ŒãŸæ˜ç¢ºãªè²¬ä»»å¢ƒç•Œ
- **ä¾å­˜æ€§æ³¨å…¥**: ãƒ†ã‚¹ã‚¿ãƒ–ãƒ«ã§æ‹¡å¼µå¯èƒ½ãªè¨­è¨ˆ
- **ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ‘ã‚¿ãƒ¼ãƒ³**: ç’°å¢ƒåˆ¥ã®æœ€é©åŒ–ã•ã‚ŒãŸã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ
- **æŠ½è±¡åŸºåº•ã‚¯ãƒ©ã‚¹**: ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®æ˜ç¢ºåŒ–

### ğŸ§ª åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
- **ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸84%**: é«˜å“è³ªãªã‚³ãƒ¼ãƒ‰ä¿è¨¼
- **160+ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹**: å…¨æ©Ÿèƒ½ã®å‹•ä½œç¢ºèª
- **ãƒ¢ãƒƒã‚¯ãƒ»ã‚¹ã‚¿ãƒ–æ´»ç”¨**: å¤–éƒ¨ä¾å­˜ã®åˆ†é›¢
- **çµ±åˆãƒ†ã‚¹ãƒˆ**: å®Ÿéš›ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®æ¤œè¨¼

### ğŸš€ CI/CD ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
- **GitHub Actions**: è‡ªå‹•åŒ–ã•ã‚ŒãŸãƒ†ã‚¹ãƒˆãƒ»ãƒ“ãƒ«ãƒ‰ãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤
- **è¤‡æ•°Pythonç‰ˆå¯¾å¿œ**: 3.9-3.13ã§ã®å‹•ä½œä¿è¨¼
- **å“è³ªã‚²ãƒ¼ãƒˆ**: ã‚«ãƒãƒ¬ãƒƒã‚¸80%ä»¥ä¸Šã®å¼·åˆ¶
- **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚­ãƒ£ãƒ³**: Banditãƒ»Safetyçµ±åˆ

### ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
- **å®Ÿè¡Œæ™‚é–“æ¸¬å®š**: è©³ç´°ãªæ€§èƒ½ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¿½è·¡**: ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡ã®æœ€é©åŒ–
- **WebDriveræ€§èƒ½**: ãƒšãƒ¼ã‚¸ãƒ­ãƒ¼ãƒ‰ãƒ»è¦ç´ æ¤œç´¢ã®è¨ˆæ¸¬
- **ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½**: é‡è¤‡ã‚¢ã‚¯ã‚»ã‚¹ã®å‰Šæ¸›

### ğŸ”” ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆæ©Ÿèƒ½
- **ã‚¨ãƒ©ãƒ¼è¿½è·¡**: åŒ…æ‹¬çš„ãªä¾‹å¤–ç›£è¦–
- **ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†**: ã‚·ã‚¹ãƒ†ãƒ å¥åº·çŠ¶æ…‹ã®å¯è¦–åŒ–
- **è‡ªå‹•ã‚¢ãƒ©ãƒ¼ãƒˆ**: ã—ãã„å€¤ãƒ™ãƒ¼ã‚¹ã®é€šçŸ¥
- **ãƒ¡ãƒ¼ãƒ«ãƒ»Slacké€šçŸ¥**: è¤‡æ•°ãƒãƒ£ãƒãƒ«å¯¾å¿œ

## ğŸ›ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ§‹æˆ

```
src/ishifuku/
â”œâ”€â”€ config.py              # çµ±ä¸€è¨­å®šç®¡ç†
â”œâ”€â”€ core.py                 # ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯
â”œâ”€â”€ optimized_core.py       # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ç‰ˆ
â”œâ”€â”€ scraping/               # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ©Ÿèƒ½
â”‚   â”œâ”€â”€ driver.py           # WebDriverç®¡ç†
â”‚   â”œâ”€â”€ parser.py           # HTMLè§£æ
â”‚   â””â”€â”€ extractor.py        # ä¾¡æ ¼æŠ½å‡º
â”œâ”€â”€ storage/                # ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–
â”‚   â”œâ”€â”€ csv_handler.py      # CSVã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
â”‚   â””â”€â”€ s3_handler.py       # S3ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
â”œâ”€â”€ utils/                  # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”‚   â”œâ”€â”€ datetime_utils.py   # æ—¥æ™‚å‡¦ç†
â”‚   â””â”€â”€ logging_utils.py    # ãƒ­ã‚°ç®¡ç†
â””â”€â”€ monitoring/             # ç›£è¦–æ©Ÿèƒ½
    â”œâ”€â”€ __init__.py         # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
    â””â”€â”€ alerting.py         # ã‚¢ãƒ©ãƒ¼ãƒˆç®¡ç†
```

## ğŸ”§ è¨­å®šç®¡ç†

### ç’°å¢ƒåˆ¥è¨­å®š

```python
# ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒ
config = ApplicationConfig.get_config("local")

# Lambdaç’°å¢ƒ  
config = ApplicationConfig.get_config("lambda")
```

### è¨­å®šé …ç›®

```python
@dataclass
class ApplicationConfig:
    scraping: ScrapingConfig      # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°è¨­å®š
    webdriver: WebDriverConfig    # WebDriverè¨­å®š
    storage: StorageConfig        # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸è¨­å®š
    log_level: str = "INFO"       # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«
    log_file: str = "logs/app.log"  # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«
    environment: str = "local"    # å®Ÿè¡Œç’°å¢ƒ
```

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬çš„ãªä½¿ç”¨æ³•

```python
from src.ishifuku.core import create_gold_price_scraper

# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ä½œæˆãƒ»å®Ÿè¡Œ
with create_gold_price_scraper() as scraper:
    success, error = scraper.scrape_and_save()
    if success:
        print("âœ… é‡‘ä¾¡æ ¼ã®å–å¾—ã«æˆåŠŸã—ã¾ã—ãŸ")
    else:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {error}")
```

### æœ€é©åŒ–ç‰ˆã®ä½¿ç”¨

```python
from src.ishifuku.optimized_core import create_optimized_scraper

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ»ç›£è¦–æ©Ÿèƒ½ä»˜ã
with create_optimized_scraper(
    environment="local",
    enable_cache=True,
    cache_ttl=300
) as scraper:
    success, error = scraper.scrape_and_save()
```

### ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ

```bash
# åŸºæœ¬ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
python -m src.main_monitoring scrape

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–
python -m src.main_monitoring scrape --no-cache

# Lambdaç’°å¢ƒç”¨
python -m src.main_monitoring scrape --environment lambda

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†
python -m src.main_monitoring cache info
python -m src.main_monitoring cache clear
python -m src.main_monitoring cache warmup

# ç›£è¦–çŠ¶æ…‹ç¢ºèª
python -m src.main_monitoring monitor
```

## ğŸ§ª ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ

### å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ

```bash
# ã‚«ãƒãƒ¬ãƒƒã‚¸ä»˜ããƒ†ã‚¹ãƒˆå®Ÿè¡Œ
python -m pytest tests/ --cov=src/ishifuku --cov-report=html -v

# ç‰¹å®šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ†ã‚¹ãƒˆ
python -m pytest tests/test_core.py -v

# ä¸¦åˆ—å®Ÿè¡Œï¼ˆé«˜é€ŸåŒ–ï¼‰
python -m pytest tests/ -n auto
```

### ãƒ†ã‚¹ãƒˆæ§‹æˆ

```
tests/
â”œâ”€â”€ test_config.py          # è¨­å®šç®¡ç†ãƒ†ã‚¹ãƒˆ
â”œâ”€â”€ test_core.py            # ã‚³ã‚¢æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
â”œâ”€â”€ test_extractor.py       # ä¾¡æ ¼æŠ½å‡ºãƒ†ã‚¹ãƒˆ
â”œâ”€â”€ test_parser.py          # HTMLè§£æãƒ†ã‚¹ãƒˆ
â”œâ”€â”€ test_driver.py          # WebDriverãƒ†ã‚¹ãƒˆ
â”œâ”€â”€ test_storage.py         # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ†ã‚¹ãƒˆ
â”œâ”€â”€ test_s3_storage.py      # S3ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ†ã‚¹ãƒˆ
â”œâ”€â”€ test_utils.py           # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ
â”œâ”€â”€ test_monitoring.py      # ç›£è¦–æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
â”œâ”€â”€ test_scrape_ishifuku.py # ãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›ãƒ†ã‚¹ãƒˆ
â””â”€â”€ test_scraping_functions.py # çµ±åˆãƒ†ã‚¹ãƒˆ
```

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½

```python
# ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹åŒ–ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ5åˆ†ï¼‰
scraper = create_optimized_scraper(
    enable_cache=True,
    cache_ttl=300  # 5åˆ†
)

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±ç¢ºèª
cache_info = scraper.get_cache_info()
print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹: {cache_info['is_valid']}")
print(f"çµŒéæ™‚é–“: {cache_info['age_seconds']}ç§’")

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
scraper.clear_cache()
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š

```python
from src.ishifuku.monitoring import PerformanceMonitor

monitor = PerformanceMonitor()
monitor.start_monitoring()

# å‡¦ç†å®Ÿè¡Œ
with monitor.measure_operation("scraping"):
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†
    pass

results = monitor.stop_monitoring()
print(f"å®Ÿè¡Œæ™‚é–“: {results['total_execution_time']:.2f}ç§’")
print(f"ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒª: {results['memory_peak']/1024/1024:.1f}MB")
```

## ğŸ”” ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š

### åŸºæœ¬ç›£è¦–

```python
from src.ishifuku.monitoring.alerting import create_monitoring_system

# ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
monitoring = create_monitoring_system()

# å®Ÿè¡Œãƒ‡ãƒ¼ã‚¿è¿½è·¡
execution_data = {
    "execution_time": 15.0,
    "memory_peak": 100000000,
    "success": True
}
monitoring.track_execution(execution_data)

# ã‚¢ãƒ©ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
alerts = monitoring.check_and_send_alerts()

# ã‚·ã‚¹ãƒ†ãƒ å¥åº·çŠ¶æ…‹
health = monitoring.get_health_status()
print(f"çŠ¶æ…‹: {health['status']}")
```

### ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«è¨­å®š

```python
# ã‚«ã‚¹ã‚¿ãƒ ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«
alert_manager.add_alert_rule({
    "name": "é«˜ã‚¨ãƒ©ãƒ¼ç‡ã‚¢ãƒ©ãƒ¼ãƒˆ",
    "condition": "error_count",
    "threshold": 5,
    "severity": "critical",
    "message": "ã‚¨ãƒ©ãƒ¼ãŒé »ç™ºã—ã¦ã„ã¾ã™"
})

alert_manager.add_alert_rule({
    "name": "é•·æ™‚é–“å®Ÿè¡Œã‚¢ãƒ©ãƒ¼ãƒˆ", 
    "condition": "execution_time",
    "threshold": 60.0,
    "severity": "warning",
    "message": "å®Ÿè¡Œæ™‚é–“ãŒé•·ã™ãã¾ã™"
})
```

### ãƒ¡ãƒ¼ãƒ«é€šçŸ¥è¨­å®š

```bash
# ç’°å¢ƒå¤‰æ•°ã§ãƒ¡ãƒ¼ãƒ«è¨­å®š
export ALERT_EMAIL_ENABLED=true
export ALERT_EMAIL_FROM=noreply@example.com
export ALERT_EMAIL_TO=admin@example.com
export SMTP_SERVER=smtp.gmail.com
export SMTP_PORT=587
export SMTP_USE_TLS=true
export SMTP_USERNAME=your_username
export SMTP_PASSWORD=your_password
```

## ğŸš€ CI/CD ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

### GitHub Actions ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

```yaml
# .github/workflows/ci-cd.yml
name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11, 3.12, 3.13]
    
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest-cov pytest-mock
    
    - name: Run tests with coverage
      run: |
        pytest tests/ \
          --cov=src/ishifuku \
          --cov-fail-under=80 \
          --junitxml=pytest-results.xml
```

### å“è³ªã‚²ãƒ¼ãƒˆ

- **ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸**: 80%ä»¥ä¸Šå¿…é ˆ
- **Linting**: flake8, black, isort
- **å‹ãƒã‚§ãƒƒã‚¯**: mypy
- **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£**: bandit, safety

## ğŸ“¦ ãƒ‡ãƒ—ãƒ­ã‚¤

### ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒ

```bash
# ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt

# è¨­å®šç¢ºèª
python -c "from src.ishifuku.config import ApplicationConfig; print(ApplicationConfig.get_config('local'))"

# å®Ÿè¡Œ
python -m src.main_monitoring scrape
```

### AWS Lambda

```bash
# Lambda ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä½œæˆ
cd lambda/
pip install -r requirements.txt -t .
zip -r ../lambda-function.zip . -x "*.pyc" "__pycache__/*"

# ãƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆAWS CLIï¼‰
aws lambda update-function-code \
  --function-name ishifuku-scraper \
  --zip-file fileb://../lambda-function.zip
```

### DockeråŒ–

```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY src/ src/
COPY tests/ tests/

# Chrome ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
RUN apt-get update && apt-get install -y \
    chromium-browser \
    chromium-chromedriver

CMD ["python", "-m", "src.main_monitoring", "scrape"]
```

## ğŸ“ˆ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ»KPI

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™

- **å®Ÿè¡Œæ™‚é–“**: å¹³å‡15-20ç§’ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡ã—ï¼‰
- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: ãƒ”ãƒ¼ã‚¯100-200MB
- **æˆåŠŸç‡**: 99%ä»¥ä¸Š
- **å¿œç­”æ™‚é–“**: ãƒšãƒ¼ã‚¸ãƒ­ãƒ¼ãƒ‰3ç§’ä»¥ä¸‹

### å“è³ªæŒ‡æ¨™

- **ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸**: 84%ï¼ˆç›®æ¨™95%ï¼‰
- **ã‚³ãƒ¼ãƒ‰è¤‡é›‘åº¦**: é–¢æ•°ã‚ãŸã‚Š10ä»¥ä¸‹
- **ãƒã‚°å¯†åº¦**: æœˆæ¬¡5ä»¶ä»¥ä¸‹
- **å¯ç”¨æ€§**: 99.9%

## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œ

#### ChromeDriverã‚¨ãƒ©ãƒ¼
```bash
# ChromeDriverãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
chromedriver --version

# æ‰‹å‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install webdriver-manager
```

#### ãƒ¡ãƒ¢ãƒªä¸è¶³
```python
# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–
config = WebDriverConfig(
    chrome_arguments=[
        "--memory-pressure-off",
        "--disable-dev-shm-usage",
        "--no-sandbox"
    ]
)
```

#### ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
```python
# ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šèª¿æ•´
config = ScrapingConfig(
    base_url="https://ishifuku.co.jp",
    timeout=30,  # 30ç§’ã«å»¶é•·
    retry_count=3
)
```

### ãƒ­ã‚°åˆ†æ

```bash
# ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ç¢ºèª
tail -f logs/scraping_error.log

# ç‰¹å®šã‚¨ãƒ©ãƒ¼ã®æ¤œç´¢
grep "TimeoutException" logs/scraping_error.log

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ­ã‚°åˆ†æ
grep "å®Ÿè¡Œæ™‚é–“" logs/scraping_info.log | tail -10
```

## ğŸ“š é–‹ç™ºã‚¬ã‚¤ãƒ‰

### æ–°æ©Ÿèƒ½è¿½åŠ æ‰‹é †

1. **è¦ä»¶å®šç¾©**: æ©Ÿèƒ½ä»•æ§˜ã®æ˜ç¢ºåŒ–
2. **è¨­è¨ˆ**: ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¸ã®çµ„ã¿è¾¼ã¿æ–¹æ¤œè¨
3. **ãƒ†ã‚¹ãƒˆä½œæˆ**: TDD ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ãƒ†ã‚¹ãƒˆå…ˆè¡Œ
4. **å®Ÿè£…**: æ—¢å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¾“ã£ãŸå®Ÿè£…
5. **ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ**: ã‚«ãƒãƒ¬ãƒƒã‚¸ç¶­æŒã®ç¢ºèª
6. **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°**: ä½¿ç”¨æ–¹æ³•ã®æ–‡æ›¸åŒ–

### ã‚³ãƒ¼ãƒ‰è¦ç´„

```python
# å‹ãƒ’ãƒ³ãƒˆå¿…é ˆ
def extract_price(text: str) -> Optional[int]:
    """ä¾¡æ ¼ã‚’æŠ½å‡º"""
    pass

# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ–‡å­—åˆ—
def scrape_data(url: str) -> Dict[str, Any]:
    """
    ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
    
    Args:
        url: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡URL
        
    Returns:
        Dict[str, Any]: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœ
        
    Raises:
        TimeoutException: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚
    """
    pass

# ãƒ­ã‚°å‡ºåŠ›
from .utils import log_info, log_error

log_info("å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
log_error("ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ", exception)
```

### ãƒ†ã‚¹ãƒˆä½œæˆæŒ‡é‡

```python
# ãƒ¢ãƒƒã‚¯æ´»ç”¨
@patch('src.ishifuku.scraping.driver.webdriver.Chrome')
def test_webdriver_creation(self, mock_chrome):
    """WebDriverä½œæˆãƒ†ã‚¹ãƒˆ"""
    mock_driver = Mock()
    mock_chrome.return_value = mock_driver
    
    factory = ChromeDriverFactory()
    driver = factory.create_driver()
    
    assert driver == mock_driver
    mock_chrome.assert_called_once()

# ä¾‹å¤–ãƒ†ã‚¹ãƒˆ
def test_error_handling(self):
    """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
    scraper = GoldPriceScraper()
    
    with pytest.raises(TimeoutException):
        scraper.scrape_with_invalid_url()
```

## ğŸ¯ ä»Šå¾Œã®æ‹¡å¼µè¨ˆç”»

### Phase 3: é«˜åº¦ãªæ©Ÿèƒ½
- **æ©Ÿæ¢°å­¦ç¿’**: ä¾¡æ ¼äºˆæ¸¬ãƒ»ç•°å¸¸æ¤œçŸ¥
- **åˆ†æ•£å‡¦ç†**: Celery/Redis ã«ã‚ˆã‚‹ä¸¦åˆ—åŒ–
- **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ **: WebSocket ã«ã‚ˆã‚‹ãƒ©ã‚¤ãƒ–æ›´æ–°
- **APIåŒ–**: FastAPI ã«ã‚ˆã‚‹ REST API æä¾›

### Phase 4: ä¼æ¥­å¯¾å¿œ
- **ãƒãƒ«ãƒãƒ†ãƒŠãƒ³ãƒˆ**: è¤‡æ•°é¡§å®¢å¯¾å¿œ
- **RBAC**: ãƒ­ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡
- **ç›£æŸ»ãƒ­ã‚°**: å®Œå…¨ãªæ“ä½œå±¥æ­´
- **SLA**: ã‚µãƒ¼ãƒ“ã‚¹ãƒ¬ãƒ™ãƒ«ä¿è¨¼

## ğŸ‘¥ è²¢çŒ®è€…ã‚¬ã‚¤ãƒ‰

### é–‹ç™ºç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/your-org/ishifuku.git
cd ishifuku

# ä»®æƒ³ç’°å¢ƒä½œæˆ
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# é–‹ç™ºä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt
pip install -r requirements-dev.txt

# pre-commit ãƒ•ãƒƒã‚¯è¨­å®š
pre-commit install
```

### ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ‰‹é †

1. **Issueä½œæˆ**: ä¿®æ­£ãƒ»æ©Ÿèƒ½è¿½åŠ ã®å†…å®¹ã‚’èª¬æ˜
2. **ãƒ–ãƒ©ãƒ³ãƒä½œæˆ**: `feature/issue-123-description`
3. **å®Ÿè£…**: ãƒ†ã‚¹ãƒˆè¾¼ã¿ã§å®Ÿè£…
4. **ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ**: å…¨ãƒ†ã‚¹ãƒˆãƒ‘ã‚¹ç¢ºèª
5. **ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆ**: è©³ç´°ãªèª¬æ˜ä»˜ãã§ä½œæˆ
6. **ãƒ¬ãƒ“ãƒ¥ãƒ¼**: ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾å¿œ
7. **ãƒãƒ¼ã‚¸**: CI/CD ãƒ‘ã‚¹å¾Œã«ãƒãƒ¼ã‚¸

## ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ MIT ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ä¸‹ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚

---

**Ishifuku Gold Price Scraper v2** - Professional Web Scraping Solution with Monitoring & CI/CD
